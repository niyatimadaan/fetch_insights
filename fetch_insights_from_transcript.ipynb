{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a40f15c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = \"\"\"\n",
    "Interviewer: Hello, thank you for joining us today. We're excited to get your feedback on our app.\n",
    "Interviewee: Hi, happy to be here. Overall, the app's been good.\n",
    "Interviewer: That's great to hear! Could you tell us more about your experience using the app?\n",
    "Interviewee: Sure. It's user-friendly, but more personalized features would enhance it.\n",
    "Interviewer: Personalization noted. What specific features would you like to see for a more fulfilling experience?\n",
    "Interviewee: Customizable notifications and tailored content based on my preferences would be fantastic.\n",
    "Interviewer: Noted. Now, are there any areas you feel could use improvement within the app?\n",
    "Interviewee: Occasionally, the app lags during peak hours. Improving its speed would be beneficial.\n",
    "Interviewer: Thank you for sharing that. We'll look into optimizing the app's performance. Any other areas?\n",
    "Interviewee: The search function could be more accurate. It sometimes misses relevant results.\n",
    "Interviewer: Understood. We'll work on refining the search algorithm. Any final thoughts or suggestions?\n",
    "Interviewee: Overall, I'm satisfied. Just a few tweaks would make the app even better.\n",
    "Interviewer: We appreciate your feedback. It's invaluable for us to enhance the app. Thank you for your time today.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7a45309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Hi, happy to be here. Overall, the app's been good.\", \"Sure. It's user-friendly, but more personalized features would enhance it.\", 'Customizable notifications and tailored content based on my preferences would be fantastic.', 'Occasionally, the app lags during peak hours. Improving its speed would be beneficial.', 'The search function could be more accurate. It sometimes misses relevant results.', \"Overall, I'm satisfied. Just a few tweaks would make the app even better.\"]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_interviewee_transcript(transcript):\n",
    "    lines = transcript.split('\\n')\n",
    "    \n",
    "    interviewee = []\n",
    "    interviewee_regex = re.compile(r'Interviewee: (.*)')\n",
    "    \n",
    "    for line in lines:\n",
    "        interviewee_match = interviewee_regex.search(line)\n",
    "        \n",
    "        if interviewee_match:\n",
    "            interviewee.append(interviewee_match.group(1))\n",
    "            \n",
    "    return interviewee\n",
    "\n",
    "interviewee_feedback = extract_interviewee_transcript(transcript)        \n",
    "print(interviewee_feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e56d50d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niyat\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "C:\\Users\\niyat\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n",
      "C:\\Users\\niyat\\AppData\\Local\\Temp/ipykernel_18244/4188998071.py:7: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\niyat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\niyat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Results:\n",
      "Hi, happy to be here. Overall, the app's been good.: {'neg': 0.0, 'neu': 0.548, 'pos': 0.452, 'compound': 0.765}\n",
      "Sure. It's user-friendly, but more personalized features would enhance it.: {'neg': 0.0, 'neu': 0.845, 'pos': 0.155, 'compound': 0.1655}\n",
      "Customizable notifications and tailored content based on my preferences would be fantastic.: {'neg': 0.0, 'neu': 0.753, 'pos': 0.247, 'compound': 0.5574}\n",
      "Occasionally, the app lags during peak hours. Improving its speed would be beneficial.: {'neg': 0.125, 'neu': 0.558, 'pos': 0.318, 'compound': 0.5367}\n",
      "The search function could be more accurate. It sometimes misses relevant results.: {'neg': 0.147, 'neu': 0.853, 'pos': 0.0, 'compound': -0.2263}\n",
      "Overall, I'm satisfied. Just a few tweaks would make the app even better.: {'neg': 0.0, 'neu': 0.637, 'pos': 0.363, 'compound': 0.6908}\n",
      "\n",
      "Information Extraction Results:\n",
      "Entities: ['peak hours']\n",
      "Noun Phrases: ['Hi', 'the app', 'It', 'more personalized features', 'it', 'Customizable notifications', 'tailored content', 'my preferences', 'the app', 'peak hours', 'its speed', 'The search function', 'It', 'relevant results', 'I', 'Just a few tweaks', 'the app']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "tokens = word_tokenize(transcript)\n",
    "tokens = [word for word in tokens if word.isalnum()]\n",
    "tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "\n",
    "sentiment_scores = [sia.polarity_scores(sentence.replace(\"Interviewee:\", \"\").strip()) for sentence in interviewee_feedback]\n",
    "\n",
    "# Information extraction\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\" \".join(interviewee_feedback))\n",
    "\n",
    "# Extracting entities and noun phrases\n",
    "entities = [ent.text for ent in doc.ents]\n",
    "noun_phrases = [chunk.text for chunk in doc.noun_chunks]\n",
    "\n",
    "# Displaying results\n",
    "print(\"Sentiment Analysis Results:\")\n",
    "for sentence, score in zip(interviewee_feedback, sentiment_scores):\n",
    "    print(f\"{sentence}: {score}\")\n",
    "\n",
    "print(\"\\nInformation Extraction Results:\")\n",
    "print(\"Entities:\", entities)\n",
    "print(\"Noun Phrases:\", noun_phrases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d31a81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary:\n",
      "the app's user-friendly, but more personalized features would enhance it . customizable notifications and tailored content based on my preferences would be fantastic .\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, TFAutoModelForSeq2SeqLM\n",
    "\n",
    "def generate_summary(transcript, model_name_or_path):\n",
    "    text = \" \".join(transcript)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "    model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)\n",
    "\n",
    "    summarization_pipeline = pipeline(\"summarization\", model=model, tokenizer=tokenizer, framework=\"tf\")\n",
    "\n",
    "    summary = summarization_pipeline(text, max_length=50, min_length=10, do_sample=False)\n",
    "\n",
    "    return summary[0]['summary_text']\n",
    "\n",
    "\n",
    "\n",
    "model_name_or_path = \"t5-small\"\n",
    "\n",
    "summary = generate_summary(interviewee_feedback, model_name_or_path)\n",
    "\n",
    "# Printing\n",
    "print(\"Generated Summary:\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f554a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
